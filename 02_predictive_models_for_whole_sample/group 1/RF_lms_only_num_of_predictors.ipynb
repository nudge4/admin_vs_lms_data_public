{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is used to find out how model performance changes with number of predictors for the LMS-only, non-first-term model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy.stats.mstats import gmean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_dir = \"~\\\\Box Sync\\\\Clickstream\\\\evaluation_results\\\\full\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"~\\\\Box Sync\\\\Clickstream\\\\data\\\\full\\\\LMS_data_final_full_new.csv\")\n",
    "predictors = [e for e in list(df.columns)[5:] if e != \"grade\"]\n",
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert pd.isnull(df).any().any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969025, 56)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ys8mz\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(698361, 56) (270664, 56)\n"
     ]
    }
   ],
   "source": [
    "train_df = df[df.strm != 2212]\n",
    "test_df = df[df.strm == 2212]\n",
    "original_test_grade = np.array(test_df.grade)\n",
    "train_df.loc[:,'grade'] = train_df.apply(lambda x: 1 if x.loc['grade'] in {'A','B','C'} else 0, axis=1)\n",
    "test_df.loc[:,'grade'] = test_df.apply(lambda x: 1 if x.loc['grade'] in {'A','B','C'} else 0, axis=1)\n",
    "print(train_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw_predictors = {\\\n",
    "# 10: 'tot_click_cnt_qrt1, tot_time_qrt1, assign_sub_cnt_qtr1, has_assign_sub_cnt_qtr1, disc_post_cnt_qtr1, disc_reply_cnt_qtr1, on_time_assign_share_qtr1, has_on_time_assign_share_qtr1, avg_depth_post_qtr1, avg_word_tot_qtr1',\n",
    "# 20: 'has_concurrent_qtr1, tot_click_cnt_qrt1c, tot_time_qrt1c, prior_disc_post_cnt, irreg_session_len_qrt1, prior_has_full, prior_tot_click_cnt, prior_tot_act_day_cnt, prior_disc_reply_cnt, avg_session_len_qrt1',\n",
    "# 30: 'prior_tot_act_wk_cnt, assign_sub_cnt_qtr1c, has_assign_sub_cnt_qtr1c, irreg_session_len_qrt1c, prior_on_time_assign_share, prior_has_on_time_assign_share, avg_session_len_qrt1c, prior_has_qtr1, prior_tot_click_cnt_qrt1, prior_tot_time',\n",
    "# 40: 'prior_tot_session_cnt, prior_disc_reply_cnt_qtr1, prior_assign_sub_cnt, prior_has_assign_sub_cnt, prior_disc_post_cnt_qtr1, prior_tot_time_qrt1, prior_avg_word_tot, prior_avg_depth_post, prior_avg_depth_post_qtr1, prior_avg_word_tot_qtr1',\n",
    "# 50: 'prior_on_time_assign_share_qtr1, prior_irreg_session_len, prior_avg_session_len, prior_irreg_session_len_qrt1, prior_avg_session_len_qrt1, prior_assign_sub_cnt_qtr1, on_time_assign_share_qtr1c, has_on_time_assign_share_qtr1c, prior_has_on_time_assign_share_qtr1, prior_has_assign_sub_cnt_qtr1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_predictors = {\\\n",
    "5: 'tot_click_cnt_qrt1, tot_time_qrt1, assign_sub_cnt_qtr1, has_assign_sub_cnt_qtr1, disc_post_cnt_qtr1',\n",
    "15: 'disc_reply_cnt_qtr1, on_time_assign_share_qtr1, has_on_time_assign_share_qtr1, avg_depth_post_qtr1, has_concurrent_qtr1, tot_click_cnt_qrt1c, avg_word_tot_qtr1, tot_time_qrt1c, prior_has_full, prior_disc_post_cnt',\n",
    "25: 'irreg_session_len_qrt1, prior_tot_click_cnt, prior_tot_act_day_cnt, prior_disc_reply_cnt, avg_session_len_qrt1, prior_tot_act_wk_cnt, assign_sub_cnt_qtr1c, has_assign_sub_cnt_qtr1c, irreg_session_len_qrt1c, avg_session_len_qrt1c',\n",
    "35: 'prior_on_time_assign_share, prior_has_on_time_assign_share, prior_has_qtr1, prior_tot_click_cnt_qrt1, prior_tot_time, prior_tot_session_cnt, prior_disc_reply_cnt_qtr1, prior_assign_sub_cnt, prior_has_assign_sub_cnt, prior_disc_post_cnt_qtr1',\n",
    "45: 'prior_tot_time_qrt1, prior_avg_word_tot, prior_avg_depth_post, prior_avg_depth_post_qtr1, prior_avg_word_tot_qtr1, prior_on_time_assign_share_qtr1, prior_has_on_time_assign_share_qtr1, prior_irreg_session_len, prior_avg_session_len, prior_irreg_session_len_qrt1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_predictors = {}\n",
    "for k,v in raw_predictors.items():\n",
    "    clean_predictors[k] = [p.strip() for p in v.split(\",\")]\n",
    "num_of_predictors = sorted(list(clean_predictors.keys()))\n",
    "clean_predictors_2 = {}\n",
    "for k,v in clean_predictors.items():\n",
    "    clean_predictors_2[k] = []\n",
    "    for n in num_of_predictors:\n",
    "        if n <= k:\n",
    "            clean_predictors_2[k] += clean_predictors[n]\n",
    "    assert len(clean_predictors_2[k]) == k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cv_folds(train, predictors, n_fold = 5):\n",
    "    folds = []\n",
    "    k_fold = StratifiedKFold(n_splits = n_fold, random_state = 12345, shuffle=True)\n",
    "    for train_indices, test_indices in k_fold.split(train, train.grade):\n",
    "        train_part = train.iloc[train_indices,:]\n",
    "        test_part = train.iloc[test_indices,:]\n",
    "        X_1 = train_part.loc[:,predictors]\n",
    "        y_1 = train_part.grade\n",
    "        X_2 = test_part.loc[:,predictors]\n",
    "        y_2 = test_part.grade\n",
    "        folds.append([(X_1.copy(),y_1.copy()),(X_2.copy(),y_2.copy())])\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_RF(rf_model, folds):\n",
    "    auc_by_fold = []\n",
    "    for f in folds:\n",
    "        X_1 = f[0][0]\n",
    "        y_1 = f[0][1]\n",
    "        X_2 = f[1][0]\n",
    "        y_2 = f[1][1]\n",
    "        rf_model.fit(X_1,y_1)\n",
    "        y_2_pred = rf_model.predict_proba(X_2)[:,1]\n",
    "        auc_by_fold.append(roc_auc_score(y_2,y_2_pred))\n",
    "    return round(np.mean(auc_by_fold),4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cw(y):\n",
    "    # Calculate the weight of each letter grade to be used in the modeling fitting procedure: the weight is inversely proportional to the square root of the frequency of the letter grade in the training sample\n",
    "    cw = Counter(y)\n",
    "    class_weight = {k:np.sqrt(cw.most_common()[0][-1]/v, dtype=np.float32) for k,v in cw.items()}\n",
    "    return class_weight # The output is a dictionary mapping letter grade to the corresponding weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 5:\n",
      "\n",
      "max_depth = 9: 0.7082\n",
      "max_depth = 10: 0.7088\n",
      "\n",
      "C-statistic = 0.7052\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n = 15:\n",
      "\n",
      "max_depth = 9: 0.7445\n",
      "max_depth = 10: 0.7467\n",
      "max_depth = 11: 0.7483\n",
      "max_depth = 12: 0.7498\n",
      "max_depth = 13: 0.7507\n",
      "\n",
      "C-statistic = 0.7621\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n = 25:\n",
      "\n",
      "max_depth = 9: 0.7461\n",
      "max_depth = 10: 0.7483\n",
      "max_depth = 11: 0.7503\n",
      "max_depth = 12: 0.752\n",
      "max_depth = 13: 0.7533\n",
      "max_depth = 14: 0.7544\n",
      "max_depth = 15: 0.7552\n",
      "\n",
      "C-statistic = 0.7706\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n = 35:\n",
      "\n",
      "max_depth = 9: 0.745\n",
      "max_depth = 10: 0.7474\n",
      "max_depth = 11: 0.7499\n",
      "max_depth = 12: 0.7517\n",
      "max_depth = 13: 0.7535\n",
      "max_depth = 14: 0.7549\n",
      "max_depth = 15: 0.7562\n",
      "max_depth = 16: 0.7572\n",
      "max_depth = 17: 0.7581\n",
      "\n",
      "C-statistic = 0.7759\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n = 45:\n",
      "\n",
      "max_depth = 9: 0.7447\n",
      "max_depth = 10: 0.7475\n",
      "max_depth = 11: 0.7499\n",
      "max_depth = 12: 0.7519\n",
      "max_depth = 13: 0.7539\n",
      "max_depth = 14: 0.7557\n",
      "max_depth = 15: 0.757\n",
      "max_depth = 16: 0.7584\n",
      "max_depth = 17: 0.7598\n",
      "max_depth = 18: 0.7607\n",
      "\n",
      "C-statistic = 0.7773\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cstat_by_num = []\n",
    "for n in sorted(list(clean_predictors_2.keys())):\n",
    "    print(\"n = {}:\\n\".format(n))\n",
    "    predictors_list = clean_predictors_2[n]\n",
    "    five_folds = create_cv_folds(train_df, predictors_list)\n",
    "    auc_by_d=[]\n",
    "    for d in range(9,36):\n",
    "        rf = RandomForestClassifier(n_estimators=200, criterion=\"entropy\", \n",
    "                                    max_depth=d,\n",
    "                                    random_state=0, n_jobs=20, max_features=\"auto\",\n",
    "                                    class_weight = calc_cw(train_df.grade))\n",
    "        auc = cross_validation_RF(rf, five_folds)\n",
    "        auc_by_d.append(auc)\n",
    "        print(\"max_depth = {0}: {1}\".format(d, auc))\n",
    "        if d > 9:\n",
    "            if auc - auc_by_d[-2] < 0.001:\n",
    "                break\n",
    "            else:\n",
    "                best_d = d\n",
    "        else:\n",
    "            best_d = d\n",
    "    rf = RandomForestClassifier(n_estimators=200, criterion=\"entropy\",\n",
    "                                max_depth=best_d,\n",
    "                                random_state=0, n_jobs=-1, max_features=\"auto\",\n",
    "                                class_weight = calc_cw(train_df.grade))\n",
    "    rf.fit(train_df.loc[:,predictors_list], train_df.grade)\n",
    "    cstat_by_num.append(round(roc_auc_score(test_df.grade, rf.predict_proba(test_df.loc[:,predictors_list])[:,1]),4))\n",
    "    print(\"\\nC-statistic = {}\".format(cstat_by_num[-1]))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7052, 0.7621, 0.7706, 0.7759, 0.7773]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cstat_by_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LMS predictor comparisons by in-person vs. online, for the five courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lms_5courses = pd.read_stata(\"~\\\\Box Sync\\\\Clickstream\\\\data\\\\full\\\\LMS_data_five_courses.dta\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ys8mz\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "online_ind_df1 = pd.read_csv(\"~\\\\Box Sync\\\\Clickstream\\\\data\\\\first\\\\course_specific_predictors_new.csv\")\n",
    "online_ind_df2 = pd.read_csv(\"~\\\\Box Sync\\\\Clickstream\\\\data\\\\full\\\\course_specific_predictors_new.csv\")\n",
    "online_ind_df = pd.concat([online_ind_df1, online_ind_df2])\n",
    "online_ind_df = online_ind_df.loc[:,['vccsid', 'strm', 'college', 'course', 'section', 'online_ind']]\n",
    "lms_5courses = lms_5courses.merge(online_ind_df, on=['vccsid', 'strm', 'college', 'course', 'section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120941, 56)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df[df.course.apply(lambda x: x in {'ENG_111','ENG_112', 'BIO_101', 'MTH_154', 'MTH_161'})]\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120941, 18)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df.loc[:,['vccsid', 'strm', 'course', 'section']].merge(lms_5courses, on=['vccsid', 'strm', 'course', 'section'], how='inner')\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_list = []\n",
    "for c in ['ENG_111','ENG_112', 'BIO_101', 'MTH_154', 'MTH_161']:\n",
    "    sub0 = new_df[new_df.course == c]\n",
    "    for i in [0,1]:\n",
    "        sub = sub0[sub0.online_ind == i].describe().drop(['strm', 'online_ind'], axis=1).loc[['mean', 'std', '50%'], :].T.reset_index()\n",
    "        sub = sub.rename(columns={'index':'predictor', '50%': 'median'})\n",
    "        sub.loc[:,'course'] = c\n",
    "        if i == 0:\n",
    "            sub.loc[:,'mode'] = \"in-person\"\n",
    "        else:\n",
    "            sub.loc[:,'mode'] = \"online\"\n",
    "        sub_list.append(sub)\n",
    "sub_merged = pd.concat(sub_list).loc[:,['predictor', 'course', 'mode', 'mean', 'median', 'std']].sort_values(['predictor', 'course', 'mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_merged_new = \\\n",
    "sub_merged[sub_merged.predictor.apply(lambda x: x in {'tot_click_cnt_qrt1', 'tot_time_qrt1', \n",
    "                                                      'avg_word_tot_qtr1', 'has_assign_sub_cnt_qtr1'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_p = []\n",
    "for v in ['mean', 'median', 'std']:\n",
    "    p1 = sub_merged_new[sub_merged_new.loc[:,'mode'] == \"in-person\"].pivot(index='course', columns='predictor', values=v).reset_index()\n",
    "    p1.loc[:,\"mode\"] = \"in-person\"\n",
    "    p2 = sub_merged_new[sub_merged_new.loc[:,'mode'] == \"online\"].pivot(index='course', columns='predictor', values=v).reset_index()\n",
    "    p2.loc[:,\"mode\"] = \"online\"\n",
    "    p = pd.concat([p1,p2]).sort_values(['course', 'mode'])\n",
    "    p = p.loc[:,['course','mode','tot_click_cnt_qrt1','tot_time_qrt1','avg_word_tot_qtr1','has_assign_sub_cnt_qtr1']]\n",
    "    all_p.append(p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_p[0].merge(all_p[1],on=['course','mode']).merge(all_p[2], on=['course', 'mode']).iloc[:,[0,1,2,6,10,3,7,11,4,8,12,5,9,13]].round(4).to_csv(results_dir + \"lms_predictors_comparison_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_merged.round(4).to_csv(results_dir + \"lms_predictor_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
